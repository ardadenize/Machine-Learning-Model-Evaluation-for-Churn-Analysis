{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ML Model comparision\n",
    "New versus traditional models performance comparision.\n",
    "\n",
    "10 different models were used in this study. These models are; \n",
    "\"XGBOOST\", \"LIGHT_GBM\", \"RANDOM_FOREST\", \"GRADIENT_BOOSTING\", \"DECISION_TREE\", \"KNN\"\n",
    ",\"LOGISTIC_REGRESSION\", \"MULTINOMINAL_NAIVEBAYES\", \"SVM\", \"GAUSSIAN_NAIVEBAYES\".\n",
    "\n",
    "Churn analysis was used in this study.\n",
    "Dataset consists of 112 column and 6218 rows.\n",
    "\n",
    "Churn results of dataset are; \n",
    "1    5399\n",
    "0    1429\n",
    "\n",
    "Therefore I used Stratified kFold cross validation, because there is a big difference between the results.\n",
    "\n",
    "We can see models performance on this task and compare them. \n",
    "Also, we can compare performance of each model with 'train test split' and 'Stratified kFold cross validation'.\n",
    "\n",
    "The result file is attached.\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tkinter import *\n",
    "import openpyxl\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import lightgbm as ltb\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from statistics import mean, stdev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    5399\n",
       "0    1429\n",
       "Name: Churn, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Reading dataset from excel file\n",
    "\n",
    "fileName = r'D:\\BiletBankProje\\dataset.xlsx'\n",
    "\n",
    "df = pd.read_excel(fileName)\n",
    "\n",
    "# Results of project for create plots\n",
    "\n",
    "fileName2 = r'D:\\BiletBankProje\\final_result.xlsx'\n",
    "\n",
    "df_final = pd.read_excel(fileName2)\n",
    "\n",
    "# Total churn results of our dataset\n",
    "df[\"Churn\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get File informations.\n",
    "\n",
    "def fileInfo():\n",
    " split_tup = os.path.splitext('D:\\BiletBankProje\\dataset.xlsx')\n",
    " file_name = split_tup[0]\n",
    " file_extension = split_tup[1]  \n",
    " print(\"File Name: \", file_name)\n",
    " print(\"File Extension: \", file_extension) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop churn values from our dataset for estimation of these values\n",
    "inputs1 = df.drop('Churn', axis='columns')\n",
    "target = df['Churn']\n",
    "inputs1\n",
    "\n",
    "# Split into \n",
    "x = inputs1.iloc[:,:112].values\n",
    "y = target\n",
    "xTrain,xTest,yTrain,yTest = train_test_split(x,y,test_size=0.2, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result method for models\n",
    "def results(yTest,yPred):\n",
    "\n",
    " \n",
    " \n",
    " print(\"\\n***********************\\n\")\n",
    " \n",
    " acc = accuracy_score(yTest,yPred)\n",
    " print('Accuracy :',acc)\n",
    " prec = precision_score(yTest,yPred)\n",
    " print('Precision :',prec)\n",
    " f1 =f1_score(yTest,yPred)\n",
    " print('F1 Score :',f1)\n",
    " recall = recall_score(yTest,yPred)\n",
    " print('Recall :',recall)\n",
    " cn = confusion_matrix(yTest,yPred)\n",
    " print('confusion_matrix :\\n',cn) \n",
    "\n",
    " print(\"\\n***********************\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters changed and tested manually for each model\n",
    "# parameters that work best f1_score are wrote\n",
    "# train test split method used for these models\n",
    "\n",
    "# Decision Tree\n",
    "\n",
    "def DecisionTree_train_test_split():\n",
    "  print('\\n Decision Tree \\n')\n",
    "  \n",
    "  model = tree.DecisionTreeClassifier(criterion=\"entropy\", splitter = \"best\",min_samples_split = len(df.columns),max_depth =len(df), min_samples_leaf=10,random_state=0)\n",
    "\n",
    "  model.fit(xTrain,yTrain)\n",
    "  yPred = model.predict(xTest)\n",
    "  scr = model.score(xTest,yTest)\n",
    " \n",
    "  results(yTest,yPred)\n",
    "\n",
    "\n",
    "# Random Forest \n",
    "\n",
    "def RandomForest_train_test_split():\n",
    "   print('\\n Random Forest \\n')\n",
    " \n",
    "   regressor = RandomForestClassifier(n_estimators=42, random_state=0,criterion= \"entropy\", bootstrap= True, min_samples_split=2)\n",
    "   regressor.fit(xTrain, yTrain)\n",
    "   yPred = regressor.predict(xTest)\n",
    "  \n",
    "   results(yTest,yPred)\n",
    "\n",
    "\n",
    "# LogisticRegression\n",
    "\n",
    "def LogisticRegressionFunction_train_test_split():\n",
    " print('\\n Logistic Regression \\n')\n",
    "\n",
    "\n",
    " lr = LogisticRegression(penalty='l2', C=1.0,solver='newton-cg')            \n",
    " lr.fit(xTrain,yTrain)\n",
    " yPred = lr.predict(xTest)\n",
    " \n",
    "\n",
    " results(yTest,yPred)\n",
    "\n",
    " # KNN\n",
    "\n",
    "def KNN_train_test_split():\n",
    " print('\\n KNN \\n')\n",
    "\n",
    " knn = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski'\n",
    " ,metric_params=None, n_jobs=1, n_neighbors=3, p=1,weights='uniform')\n",
    "\n",
    "\n",
    " knn.fit(xTrain,yTrain)\n",
    " yPred = knn.predict(xTest)\n",
    "\n",
    "\n",
    " results(yTest,yPred)\n",
    "\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "\n",
    "def GaussianNaiveBayesClass_train_test_split(): \n",
    " print('\\n Gaussian NaiveBayes \\n')\n",
    " NaiveBayes = GaussianNB(var_smoothing=0.01)\n",
    " NaiveBayes.fit(xTrain,yTrain)\n",
    " NaiveBayes.fit(xTrain, yTrain)\n",
    " yPred = NaiveBayes.predict(xTest)\n",
    "\n",
    "\n",
    " results(yTest,yPred)\n",
    "\n",
    "\n",
    "\n",
    "# Multinominal Naive Bayes\n",
    "\n",
    "def MultiNaiveBayesClass_train_test_split(): \n",
    " print('\\n Multinominal NaiveBayes \\n')\n",
    " NaiveBayesM = MultinomialNB()\n",
    " NaiveBayesM.fit(xTrain,yTrain)\n",
    " NaiveBayesM.fit(xTrain, yTrain)\n",
    " yPred = NaiveBayesM.predict(xTest)\n",
    " \n",
    "\n",
    " results(yTest,yPred)\n",
    "\n",
    "def SVM_train_test_split(): \n",
    " print('\\n Support Vector Machine \\n')\n",
    "\n",
    " clf = svm.SVC(kernel=y,gamma=x)\n",
    " \n",
    " clf.fit(xTrain, yTrain)\n",
    " yPred = clf.predict(xTest)\n",
    "\n",
    " results(yTest,yPred)\n",
    "\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    " \n",
    "def GradientBoostingClass_train_test_split(): \n",
    " print('\\n Gradient Boosting Classifierxx\\n')\n",
    "\n",
    " gradient_booster = GradientBoostingClassifier(loss=\"deviance\",learning_rate=0.1, \n",
    " n_estimators=100,max_depth=3, min_samples_split=2, min_samples_leaf=1, subsample=10, \n",
    " random_state=None,max_features=None,warm_start=False)\n",
    "\n",
    " gradient_booster.fit(xTrain, yTrain)\n",
    " yPred = gradient_booster.predict(xTest)\n",
    " \n",
    "\n",
    " results(yTest,yPred)\n",
    "\n",
    " # Light GBM Classifier\n",
    "\n",
    "def LGBMClass_train_test_split(): \n",
    " print('\\n Light GBM Classifier \\n')\n",
    " \n",
    " lgmbc = ltb.LGBMClassifier(boosting_type='gbdt')\n",
    " \n",
    " lgmbc.fit(xTrain,yTrain)\n",
    " yPred = lgmbc.predict(xTest)\n",
    " \n",
    "\n",
    " results(yTest,yPred)\n",
    "\n",
    "# XgBoost Classifier\n",
    "\n",
    "def XgBoostClass_train_test_split(): \n",
    " print('\\n XgBoost Classifier \\n')\n",
    "\n",
    " xgbc = xgb.XGBClassifier(booster=\"gbtree\",max_depth=8,gamma=2)\n",
    " xgbc.fit(xTrain,yTrain)\n",
    " yPred = xgbc.predict(xTest)\n",
    " \n",
    "\n",
    " results(yTest,yPred)\n",
    "\n",
    "\n",
    "def runAll_train_test_split():\n",
    "   DecisionTree_train_test_split()\n",
    "   RandomForest_train_test_split()\n",
    "   LogisticRegressionFunction_train_test_split()\n",
    "   KNN_train_test_split()\n",
    "   GaussianNaiveBayesClass_train_test_split()\n",
    "   MultiNaiveBayesClass_train_test_split()\n",
    "   SVM_train_test_split()\n",
    "   GradientBoostingClass_train_test_split()\n",
    "   LGBMClass_train_test_split()\n",
    "   XgBoostClass_train_test_split()\n",
    "\n",
    "# Accuracy and f1 scores line charts\n",
    "\n",
    "def lineChart():\n",
    " accuracy = df_final[['Score']]\n",
    " f1_score1 = df_final[['F1_Score']]\n",
    " algorithm = [\"XGBOOST\",\"LIGHT_GBM\",\"RANDOM_FOREST\",\"GRADIENT_BOOSTING\",\"DECISION_TREE\",\"KNN\"\n",
    " ,\"LOGISTIC_REGRESSION\",\"MULTINOMINAL_NAIVEBAYES\",\"SVM\",\"GAUSSIAN_NAIVEBAYES\"]\n",
    " plt.figure(figsize=[25,12.5])\n",
    "\n",
    "\n",
    " plt.plot(algorithm,accuracy,linewidth=4.0) #adds the line\n",
    " plt.grid() #adds a grid to the plot\n",
    " plt.plot(algorithm,f1_score1,linewidth=4.0,color='red') #adds the line\n",
    " plt.grid() #adds a grid to the plot\n",
    " plt.ylabel('F1_Score, Accuracy',fontsize=16) #xlabel\n",
    " plt.xlabel('Algorithms\\n Red Line: F1 Score \\n Blue Line: Accuracy',fontsize=16) #ylabel\n",
    "\n",
    " plt.figure(figsize=[50,25])\n",
    "\n",
    " plt.subplot(2,2,1) \n",
    " plt.plot(algorithm,accuracy,linewidth=4.0) #adds the line\n",
    " plt.grid() #adds a grid to the plot\n",
    " plt.ylabel('Accuracy',fontsize=16) #xlabel\n",
    " plt.xlabel('Algorithms',fontsize=16) #ylabel\n",
    " plt.subplot(2,2,2)\n",
    " plt.plot(algorithm,f1_score1,linewidth=4.0,color='red') #adds the line\n",
    " plt.grid() #adds a grid to the plot\n",
    " plt.ylabel('F1_Score',fontsize=16) #xlabel\n",
    " plt.xlabel('Algorithms',fontsize=16) #ylabel\n",
    "\n",
    "\n",
    "# Accuracy and f1 scores box charts\n",
    "\n",
    "def bar_chart():\n",
    " accuracy = df_final[['Score']].squeeze()\n",
    " f1_score1 = df_final[['F1_Score']].squeeze()\n",
    " algorithm = [\"XGBOOST\",\"LIGHT_GBM\",\"RANDOM_FOREST\",\"GRADIENT_BOOSTING\",\"DECISION_TREE\",\"KNN\"\n",
    " ,\"LOGISTIC_REGRESSION\",\"MULTINOMINAL_NAIVEBAYES\",\"SVM\",\"GAUSSIAN_NAIVEBAYES\"]\n",
    "\n",
    " fig = plt.figure(figsize = (25, 12.5))\n",
    " \n",
    " plt.bar(algorithm, accuracy, color ='maroon',width = 0.4)\n",
    " \n",
    " plt.ylabel('Accuracy',fontsize=16) #xlabel\n",
    " plt.xlabel('Algorithms',fontsize=16) #ylabel\n",
    " plt.title(\"Accuracy in different algorithms\",fontsize=16)\n",
    " plt.show()\n",
    "\n",
    " fig = plt.figure(figsize = (25, 12.5))\n",
    " \n",
    " plt.bar(algorithm, f1_score1, color ='blue',width = 0.4)\n",
    " \n",
    " plt.ylabel('F1 Score',fontsize=16) #xlabel\n",
    " plt.xlabel('Algorithms',fontsize=16) #ylabel\n",
    " plt.title(\"F1 Score in different algorithms\",fontsize=16)\n",
    " \n",
    " plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling for input features.\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "\n",
    "# Create StratifiedKFold object.\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "lst_f1_stratified = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results of Stratifed Cross Validation\n",
    "\n",
    "def resultSkf(lst_f1_stratified):\n",
    " print(\"\\n***********************\\n\")\t\n",
    " print('List of possible F1_score:', lst_f1_stratified)\n",
    " print('\\nMaximum F1_score That can be obtained from this model is:',\n",
    "\tmax(lst_f1_stratified)*100, '%')\n",
    " print('\\nMinimum F1_score:',\n",
    "\tmin(lst_f1_stratified)*100, '%')\n",
    " print('\\nOverall F1_score:',\n",
    "\tmean(lst_f1_stratified)*100, '%')\n",
    " print('\\nStandard Deviation is:', stdev(lst_f1_stratified))\n",
    " print(\"\\n***********************\\n\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters changed and tested manually for each model\n",
    "# parameters that work best f1_score are wrote\n",
    "# Stratified Cross Validation used for these models\n",
    "\n",
    "# Decision Tree\n",
    "\n",
    "def DecisionTree_skf():\n",
    " model = tree.DecisionTreeClassifier(criterion=\"entropy\", splitter = \"best\",min_samples_split = len(df.columns),\n",
    " max_depth =len(df), min_samples_leaf=10,random_state=0)\n",
    "\n",
    " for train_index, test_index in skf.split(x, y):\n",
    "  x_train_fold, x_test_fold = x_scaled[train_index], x_scaled[test_index]\n",
    "  y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "  model.fit(x_train_fold, y_train_fold)\n",
    "  yPred = model.predict(x_test_fold)\n",
    "  lst_f1_stratified.append(f1_score(y_test_fold, yPred))\n",
    "\n",
    " resultSkf(lst_f1_stratified)\n",
    "\n",
    "\n",
    "\n",
    "def RandomForest_skf():\n",
    " model = RandomForestClassifier(n_estimators=42, random_state=0,criterion= \"entropy\", \n",
    " bootstrap= True, min_samples_split=2)\n",
    "\n",
    " for train_index, test_index in skf.split(x, y):\n",
    "  x_train_fold, x_test_fold = x_scaled[train_index], x_scaled[test_index]\n",
    "  y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "  model.fit(x_train_fold, y_train_fold)\n",
    "  yPred = model.predict(x_test_fold)\n",
    "  lst_f1_stratified.append(f1_score(y_test_fold, yPred))\n",
    "\n",
    "\n",
    " resultSkf(lst_f1_stratified)\n",
    "\n",
    "\n",
    "def LogisticRegressionFunction_skf():\n",
    " model = LogisticRegression() \n",
    "\n",
    " for train_index, test_index in skf.split(x, y):\n",
    "  x_train_fold, x_test_fold = x_scaled[train_index], x_scaled[test_index]\n",
    "  y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "  model.fit(x_train_fold, y_train_fold)\n",
    "  yPred = model.predict(x_test_fold)\n",
    "  lst_f1_stratified.append(f1_score(y_test_fold, yPred))\n",
    "\n",
    "\n",
    " resultSkf(lst_f1_stratified)\n",
    "\n",
    "\n",
    "\n",
    "def KNN_skf():\n",
    " model = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski'\n",
    " ,metric_params=None, n_jobs=1, n_neighbors=3, p=1,weights='uniform')\n",
    "\n",
    " for train_index, test_index in skf.split(x, y):\n",
    "  x_train_fold, x_test_fold = x_scaled[train_index], x_scaled[test_index]\n",
    "  y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "  model.fit(x_train_fold, y_train_fold)\n",
    "  yPred = model.predict(x_test_fold)\n",
    "  lst_f1_stratified.append(f1_score(y_test_fold, yPred))\n",
    "\n",
    "\n",
    " resultSkf(lst_f1_stratified)\n",
    "\n",
    "\n",
    "\n",
    "def GaussianNaiveBayesClass_skf():\n",
    " model = GaussianNB(var_smoothing=0.01)\n",
    "\n",
    " for train_index, test_index in skf.split(x, y):\n",
    "  x_train_fold, x_test_fold = x_scaled[train_index], x_scaled[test_index]\n",
    "  y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "  model.fit(x_train_fold, y_train_fold)\n",
    "  yPred = model.predict(x_test_fold)\n",
    "  lst_f1_stratified.append(f1_score(y_test_fold, yPred))\n",
    "\n",
    "\n",
    " resultSkf(lst_f1_stratified)\n",
    "\n",
    "\n",
    "\n",
    "def MultiNaiveBayesClass_skf():\n",
    " model = MultinomialNB()\n",
    "\n",
    " for train_index, test_index in skf.split(x, y):\n",
    "  x_train_fold, x_test_fold = x_scaled[train_index], x_scaled[test_index]\n",
    "  y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "  model.fit(x_train_fold, y_train_fold)\n",
    "  yPred = model.predict(x_test_fold)\n",
    "  lst_f1_stratified.append(f1_score(y_test_fold, yPred))\n",
    "\n",
    "\n",
    " resultSkf(lst_f1_stratified)\n",
    "\n",
    "\n",
    "\n",
    "def SVM_skf():\n",
    " model = svm.SVC(kernel=y,gamma=x)\n",
    "\n",
    " for train_index, test_index in skf.split(x, y):\n",
    "  x_train_fold, x_test_fold = x_scaled[train_index], x_scaled[test_index]\n",
    "  y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "  model.fit(x_train_fold, y_train_fold)\n",
    "  yPred = model.predict(x_test_fold)\n",
    "  lst_f1_stratified.append(f1_score(y_test_fold, yPred))\n",
    "\n",
    "\n",
    " resultSkf(lst_f1_stratified)\n",
    "\n",
    "\n",
    "\n",
    "def GradientBoostingClass_skf():\n",
    " model = GradientBoostingClassifier(loss=\"deviance\",learning_rate=0.1, \n",
    " n_estimators=100,max_depth=3, min_samples_split=2, min_samples_leaf=1, subsample=10, \n",
    " random_state=None,max_features=None,warm_start=False)\n",
    "\n",
    " for train_index, test_index in skf.split(x, y):\n",
    "  x_train_fold, x_test_fold = x_scaled[train_index], x_scaled[test_index]\n",
    "  y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "  model.fit(x_train_fold, y_train_fold)\n",
    "  yPred = model.predict(x_test_fold)\n",
    "  lst_f1_stratified.append(f1_score(y_test_fold, yPred))\n",
    "\n",
    "\n",
    " resultSkf(lst_f1_stratified)\n",
    "\n",
    "\n",
    "\n",
    "def LGBMClass_skf():\n",
    " model = ltb.LGBMClassifier(boosting_type='gbdt')\n",
    "\n",
    " for train_index, test_index in skf.split(x, y):\n",
    "  x_train_fold, x_test_fold = x_scaled[train_index], x_scaled[test_index]\n",
    "  y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "  model.fit(x_train_fold, y_train_fold)\n",
    "  yPred = model.predict(x_test_fold)\n",
    "  lst_f1_stratified.append(f1_score(y_test_fold, yPred))\n",
    "\n",
    "\n",
    " resultSkf(lst_f1_stratified)\n",
    "\n",
    "\n",
    "\n",
    "def XgBoostClass_skf():\n",
    " model = xgb.XGBClassifier(booster=\"gbtree\",max_depth=8,gamma=2)\n",
    "\n",
    " for train_index, test_index in skf.split(x, y):\n",
    "  x_train_fold, x_test_fold = x_scaled[train_index], x_scaled[test_index]\n",
    "  y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "  model.fit(x_train_fold, y_train_fold)\n",
    "  yPred = model.predict(x_test_fold)\n",
    "  lst_f1_stratified.append(f1_score(y_test_fold, yPred))\n",
    "\n",
    "\n",
    " resultSkf(lst_f1_stratified)\n",
    "\n",
    "\n",
    "\n",
    "def runAll_skf():\n",
    "   DecisionTree_skf()\n",
    "   RandomForest_skf()\n",
    "   #LogisticRegressionFunction_skf()\n",
    "   KNN_skf()\n",
    "   GaussianNaiveBayesClass_skf()\n",
    "   MultiNaiveBayesClass_skf()\n",
    "   SVM_skf()\n",
    "   GradientBoostingClass_skf()\n",
    "   LGBMClass_skf()\n",
    "   XgBoostClass_skf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def menu():\n",
    "     print (' '\n",
    "      '\\n1 - Enter \"1\" for Decision Tree\\n'\n",
    "        '2 - Enter \"2\" for Random Forest\\n'\n",
    "        '4 - Enter \"3\" for KNN\\n'\n",
    "        '5 - Enter \"4\" for Gaussian Naive Bayes\\n'\n",
    "        '6 - Enter \"5\" for Multinominal Naive Bayes\\n'\n",
    "        '7 - Enter \"6\" for Support Vector Machine\\n'\n",
    "        '8 - Enter \"7\" for Gradient Boosting Classifier\\n'\n",
    "        '9 - Enter \"8\" for Light GBM Classifier\\n'\n",
    "        '10 - Enter \"9\" for XgBoost Classifier\\n'\n",
    "        '11 - Enter \"10\" for Run all Algorithms\\n'\n",
    "        '12 - Enter \"11\" for Show Line Charts\\n'\n",
    "        '13 - Enter \"12\" for Show Box Charts\\n'\n",
    "        '0 - Enter \"0\" for Exit\\n\\n')\n",
    "def menu2():\n",
    "     print (' '\n",
    "      '\\n1 - Enter \"1\" for Train Test Split\\n'\n",
    "        '2 - Enter \"2\" for Cross Validation (Stratified kFold)\\n')\n",
    "\n",
    "def options():\n",
    " flag = True\n",
    " while flag:\n",
    "    fileInfo() \n",
    "    menu()\n",
    "    option = int(input('Chose a model...'))\n",
    "    \n",
    "   \n",
    "    if option == 1:\n",
    "     option2 = int(input('Chose an option...\\n '))\n",
    "     menu2()\n",
    "     if option2 == 1:\n",
    "         DecisionTree_train_test_split()\n",
    "         continue\n",
    "     elif option2 == 2:\n",
    "         DecisionTree_skf()\n",
    "         continue\n",
    "     else:\n",
    "        print ('Invalid choice')\n",
    "        continue\n",
    "\n",
    "    \n",
    "    elif option == 2:\n",
    "     option2 = int(input('Chose an option...\\n '))\n",
    "     menu2()\n",
    "     if option2 == 1:\n",
    "         RandomForest_train_test_split()\n",
    "         continue\n",
    "     elif option2 == 2:\n",
    "         RandomForest_skf()\n",
    "         continue\n",
    "     else:\n",
    "        print ('Invalid choice')\n",
    "        continue\n",
    "\n",
    "\n",
    "    elif option == 3:\n",
    "     option2 = int(input('Chose an option...\\n '))\n",
    "     menu2()\n",
    "     if option2 == 1:\n",
    "         KNN_train_test_split()\n",
    "         continue\n",
    "     elif option2 == 2:\n",
    "           KNN_skf()\n",
    "           continue\n",
    "     else:\n",
    "        print ('Invalid choice')\n",
    "        continue\n",
    "\n",
    "\n",
    "    elif option == 4:\n",
    "     option2 = int(input('Chose an option...\\n '))\n",
    "     menu2()\n",
    "     if option2 == 1:\n",
    "         GaussianNaiveBayesClass_train_test_split()\n",
    "         continue\n",
    "     elif option2 == 2:\n",
    "           GaussianNaiveBayesClass_skf()\n",
    "           continue\n",
    "     else:\n",
    "        print ('Invalid choice')\n",
    "        continue\n",
    "\n",
    "\n",
    "    elif option == 5:\n",
    "     option2 = int(input('Chose an option...\\n '))\n",
    "     menu2()\n",
    "     if option2 == 1:\n",
    "         MultiNaiveBayesClass_train_test_split()\n",
    "         continue\n",
    "     elif option2 == 2:\n",
    "           MultiNaiveBayesClass_skf()\n",
    "           continue\n",
    "     else:\n",
    "        print ('Invalid choice')\n",
    "        continue\n",
    "\n",
    "\n",
    "    elif option == 6:\n",
    "     option2 = int(input('Chose an option...\\n '))\n",
    "     menu2()\n",
    "     if option2 == 1:\n",
    "         SVM_train_test_split()\n",
    "         continue\n",
    "     elif option2 == 2:\n",
    "           SVM_skf()\n",
    "           continue\n",
    "     else:\n",
    "        print ('Invalid choice')\n",
    "        continue\n",
    "\n",
    "\n",
    "    elif option == 7:\n",
    "     option2 = int(input('Chose an option...\\n '))\n",
    "     menu2()\n",
    "     if option2 == 1:\n",
    "         GradientBoostingClass_train_test_split()\n",
    "         continue\n",
    "     elif option2 == 2:\n",
    "           GradientBoostingClass_skf()\n",
    "           continue\n",
    "     else:\n",
    "        print ('Invalid choice')\n",
    "        continue\n",
    "\n",
    "\n",
    "    elif option == 8:\n",
    "     option2 = int(input('Chose an option...\\n '))\n",
    "     menu2()\n",
    "     if option2 == 1:\n",
    "         LGBMClass_train_test_split()\n",
    "         continue\n",
    "     elif option2 == 2:\n",
    "           LGBMClass_train_test_split()\n",
    "           continue\n",
    "     else:\n",
    "        print ('Invalid choice')\n",
    "        continue\n",
    "\n",
    "\n",
    "    elif option == 9:     \n",
    "     option2 = int(input('Chose an option...\\n '))\n",
    "     menu2()\n",
    "     if option2 == 1:\n",
    "         XgBoostClass_train_test_split()\n",
    "         continue\n",
    "     elif option2 == 2:\n",
    "           XgBoostClass_skf()\n",
    "           continue\n",
    "     else:\n",
    "        print ('Invalid choice')\n",
    "        continue\n",
    "\n",
    "\n",
    "    elif option == 10:\n",
    "     option2 = int(input('Chose an option...\\n '))\n",
    "     menu2()\n",
    "     if option2 == 1:\n",
    "         runAll_train_test_split()\n",
    "         continue\n",
    "     elif option2 == 2:\n",
    "           runAll_skf()\n",
    "           continue\n",
    "     else:\n",
    "        print ('Invalid choice')\n",
    "        continue\n",
    "\n",
    "\n",
    "    elif option == 11:\n",
    "         lineChart()\n",
    "         continue\n",
    "    elif option == 13:\n",
    "         bar_chart()\n",
    "         continue\n",
    "    elif option == 0:\n",
    "         print('Exit...')\n",
    "         break\n",
    "\n",
    "    else:\n",
    "        print ('Invalid choice')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        options()\n",
    "    except ValueError as error:\n",
    "        print('Error! Please enter a valid number...')\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name:  D:\\BiletBankProje\\dataset\n",
      "File Extension:  .xlsx\n",
      " \n",
      "1 - Enter \"1\" for Decision Tree\n",
      "2 - Enter \"2\" for Random Forest\n",
      "3- Enter \"3\" for Logistic Regression\n",
      "4 - Enter \"4\" for KNN\n",
      "5 - Enter \"5\" for Gaussian Naive Bayes\n",
      "6 - Enter \"6\" for Multinominal Naive Bayes\n",
      "7 - Enter \"7\" for Support Vector Machine\n",
      "8 - Enter \"8\" for Gradient Boosting Classifier\n",
      "9 - Enter \"9\" for Light GBM Classifier\n",
      "10 - Enter \"10\" for XgBoost Classifier\n",
      "11 - Enter \"11\" for Run all Algorithms\n",
      "12 - Enter \"12\" for Show Line Charts\n",
      "13 - Enter \"13\" for Show Box Charts\n",
      "0 - Enter \"0\" for Exit\n",
      "\n",
      "\n",
      " \n",
      "1 - Enter \"1\" for Train Test Split\n",
      "2 - Enter \"2\" for Cross Validation (Stratified kFold)\n",
      "\n",
      "\n",
      "***********************\n",
      "\n",
      "List of possible F1_score: [0.9710550887021475, 0.9592417061611374, 0.968342644320298, 0.9740259740259741, 0.9719626168224299, 0.9593956562795091, 0.9721189591078068, 0.9719626168224299, 0.9672591206735267, 0.9766136576239476, 0.9666666666666667, 0.9504761904761904, 0.9644859813084113, 0.9676823638042474, 0.9651376146788991, 0.9549718574108818, 0.9673202614379085, 0.9652014652014653, 0.9599254426840634, 0.9750231267345051]\n",
      "\n",
      "Maximum F1_score That can be obtained from this model is: 97.66136576239475 %\n",
      "\n",
      "Minimum F1_score: 95.04761904761904 %\n",
      "\n",
      "Overall F1_score: 96.64434505471223 %\n",
      "\n",
      "Standard Deviation is: 0.00685648603696335\n",
      "\n",
      "***********************\n",
      "\n",
      "\n",
      "***********************\n",
      "\n",
      "List of possible F1_score: [0.9710550887021475, 0.9592417061611374, 0.968342644320298, 0.9740259740259741, 0.9719626168224299, 0.9593956562795091, 0.9721189591078068, 0.9719626168224299, 0.9672591206735267, 0.9766136576239476, 0.9666666666666667, 0.9504761904761904, 0.9644859813084113, 0.9676823638042474, 0.9651376146788991, 0.9549718574108818, 0.9673202614379085, 0.9652014652014653, 0.9599254426840634, 0.9750231267345051, 0.9710550887021475, 0.9592417061611374, 0.968342644320298, 0.9740259740259741, 0.9719626168224299, 0.9593956562795091, 0.9721189591078068, 0.9719626168224299, 0.9672591206735267, 0.9766136576239476]\n",
      "\n",
      "Maximum F1_score That can be obtained from this model is: 97.66136576239475 %\n",
      "\n",
      "Minimum F1_score: 95.04761904761904 %\n",
      "\n",
      "Overall F1_score: 96.73615683827218 %\n",
      "\n",
      "Standard Deviation is: 0.006562868264465132\n",
      "\n",
      "***********************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***********************\n",
      "\n",
      "List of possible F1_score: [0.9710550887021475, 0.9592417061611374, 0.968342644320298, 0.9740259740259741, 0.9719626168224299, 0.9593956562795091, 0.9721189591078068, 0.9719626168224299, 0.9672591206735267, 0.9766136576239476, 0.9666666666666667, 0.9504761904761904, 0.9644859813084113, 0.9676823638042474, 0.9651376146788991, 0.9549718574108818, 0.9673202614379085, 0.9652014652014653, 0.9599254426840634, 0.9750231267345051, 0.9710550887021475, 0.9592417061611374, 0.968342644320298, 0.9740259740259741, 0.9719626168224299, 0.9593956562795091, 0.9721189591078068, 0.9719626168224299, 0.9672591206735267, 0.9766136576239476, 0.9286343612334802, 0.9165186500888101, 0.9306409130816506, 0.9275618374558304, 0.9271290605794555, 0.9260563380281691, 0.9379432624113475, 0.9288194444444444, 0.9219982471516214, 0.9295774647887324]\n",
      "\n",
      "Maximum F1_score That can be obtained from this model is: 97.66136576239475 %\n",
      "\n",
      "Minimum F1_score: 91.651865008881 %\n",
      "\n",
      "Overall F1_score: 95.73931657686299 %\n",
      "\n",
      "Standard Deviation is: 0.018572592248817833\n",
      "\n",
      "***********************\n",
      "\n",
      "\n",
      "***********************\n",
      "\n",
      "List of possible F1_score: [0.9710550887021475, 0.9592417061611374, 0.968342644320298, 0.9740259740259741, 0.9719626168224299, 0.9593956562795091, 0.9721189591078068, 0.9719626168224299, 0.9672591206735267, 0.9766136576239476, 0.9666666666666667, 0.9504761904761904, 0.9644859813084113, 0.9676823638042474, 0.9651376146788991, 0.9549718574108818, 0.9673202614379085, 0.9652014652014653, 0.9599254426840634, 0.9750231267345051, 0.9710550887021475, 0.9592417061611374, 0.968342644320298, 0.9740259740259741, 0.9719626168224299, 0.9593956562795091, 0.9721189591078068, 0.9719626168224299, 0.9672591206735267, 0.9766136576239476, 0.9286343612334802, 0.9165186500888101, 0.9306409130816506, 0.9275618374558304, 0.9271290605794555, 0.9260563380281691, 0.9379432624113475, 0.9288194444444444, 0.9219982471516214, 0.9295774647887324, 0.9426751592356687, 0.9179440937781785, 0.9305680793507665, 0.9385171790235082, 0.931159420289855, 0.9361313868613138, 0.9346642468239563, 0.9344115004492362, 0.9313113291703836, 0.932249322493225]\n",
      "\n",
      "Maximum F1_score That can be obtained from this model is: 97.66136576239475 %\n",
      "\n",
      "Minimum F1_score: 91.651865008881 %\n",
      "\n",
      "Overall F1_score: 95.25071669644257 %\n",
      "\n",
      "Standard Deviation is: 0.019485893613329332\n",
      "\n",
      "***********************\n",
      "\n",
      "\n",
      "***********************\n",
      "\n",
      "List of possible F1_score: [0.9710550887021475, 0.9592417061611374, 0.968342644320298, 0.9740259740259741, 0.9719626168224299, 0.9593956562795091, 0.9721189591078068, 0.9719626168224299, 0.9672591206735267, 0.9766136576239476, 0.9666666666666667, 0.9504761904761904, 0.9644859813084113, 0.9676823638042474, 0.9651376146788991, 0.9549718574108818, 0.9673202614379085, 0.9652014652014653, 0.9599254426840634, 0.9750231267345051, 0.9710550887021475, 0.9592417061611374, 0.968342644320298, 0.9740259740259741, 0.9719626168224299, 0.9593956562795091, 0.9721189591078068, 0.9719626168224299, 0.9672591206735267, 0.9766136576239476, 0.9286343612334802, 0.9165186500888101, 0.9306409130816506, 0.9275618374558304, 0.9271290605794555, 0.9260563380281691, 0.9379432624113475, 0.9288194444444444, 0.9219982471516214, 0.9295774647887324, 0.9426751592356687, 0.9179440937781785, 0.9305680793507665, 0.9385171790235082, 0.931159420289855, 0.9361313868613138, 0.9346642468239563, 0.9344115004492362, 0.9313113291703836, 0.932249322493225, 0.902946273830156, 0.890829694323144, 0.9017241379310345, 0.9064377682403434, 0.9092480553154709, 0.9067131647776809, 0.9201053555750657, 0.9108391608391608, 0.9000853970964987, 0.9067357512953367]\n",
      "\n",
      "Maximum F1_score That can be obtained from this model is: 97.66136576239475 %\n",
      "\n",
      "Minimum F1_score: 89.0829694323144 %\n",
      "\n",
      "Overall F1_score: 94.46837184574196 %\n",
      "\n",
      "Standard Deviation is: 0.025208360830712206\n",
      "\n",
      "***********************\n",
      "\n",
      "\n",
      "***********************\n",
      "\n",
      "List of possible F1_score: [0.9710550887021475, 0.9592417061611374, 0.968342644320298, 0.9740259740259741, 0.9719626168224299, 0.9593956562795091, 0.9721189591078068, 0.9719626168224299, 0.9672591206735267, 0.9766136576239476, 0.9666666666666667, 0.9504761904761904, 0.9644859813084113, 0.9676823638042474, 0.9651376146788991, 0.9549718574108818, 0.9673202614379085, 0.9652014652014653, 0.9599254426840634, 0.9750231267345051, 0.9710550887021475, 0.9592417061611374, 0.968342644320298, 0.9740259740259741, 0.9719626168224299, 0.9593956562795091, 0.9721189591078068, 0.9719626168224299, 0.9672591206735267, 0.9766136576239476, 0.9286343612334802, 0.9165186500888101, 0.9306409130816506, 0.9275618374558304, 0.9271290605794555, 0.9260563380281691, 0.9379432624113475, 0.9288194444444444, 0.9219982471516214, 0.9295774647887324, 0.9426751592356687, 0.9179440937781785, 0.9305680793507665, 0.9385171790235082, 0.931159420289855, 0.9361313868613138, 0.9346642468239563, 0.9344115004492362, 0.9313113291703836, 0.932249322493225, 0.902946273830156, 0.890829694323144, 0.9017241379310345, 0.9064377682403434, 0.9092480553154709, 0.9067131647776809, 0.9201053555750657, 0.9108391608391608, 0.9000853970964987, 0.9067357512953367, 0.9090909090909091, 0.8952879581151831, 0.9051724137931034, 0.9098712446351932, 0.9153713298791019, 0.9057908383751081, 0.9173194081810271, 0.9098712446351932, 0.9039451114922813, 0.9072164948453608]\n",
      "\n",
      "Maximum F1_score That can be obtained from this model is: 97.66136576239475 %\n",
      "\n",
      "Minimum F1_score: 89.0829694323144 %\n",
      "\n",
      "Overall F1_score: 93.94280008641091 %\n",
      "\n",
      "Standard Deviation is: 0.02676643472325392\n",
      "\n",
      "***********************\n",
      "\n",
      "Error! Please enter a valid number...\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ef593524384be0b3311a9de464694faadb5268e51a58eec4bd74a0352c14c813"

   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
